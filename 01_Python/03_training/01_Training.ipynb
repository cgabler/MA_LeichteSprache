{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information\n",
    "\n",
    "Code was run on Colab for performance and connection reasons   \n",
    "Adjustments might be needed  \n",
    "\n",
    "\n",
    "\n",
    "Training Code taken and changed from:  \n",
    "https://github.com/MiriUll/Language-Models-German-Simplification/tree/main  \n",
    "@inproceedings{anschutz-etal-2023-language,\n",
    "  title = \"Language Models for {G}erman Text Simplification: Overcoming Parallel Data Scarcity through Style-specific Pre-training\",\n",
    "  author = {Ansch{\"u}tz, Miriam and Oehms, Joshua and Wimmer, Thomas and Jezierski, Bart{\\l}omiej and Groh, Georg},\n",
    "  booktitle = \"Findings of the Association for Computational Linguistics: ACL 2023\",\n",
    "  month = jul,\n",
    "  year = \"2023\",\n",
    "  address = \"Toronto, Canada\",\n",
    "  publisher = \"Association for Computational Linguistics\",\n",
    "  url = \"https://aclanthology.org/2023.findings-acl.74\",\n",
    "  pages = \"1147--1158\",\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "utils and metrics.py from git needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers transformers[sentencepiece] datasets transformers[torch] evaluate textstat\n",
    "\n",
    "#needed for simctg\n",
    "!pip install wheel setuptools pip --upgrade\n",
    "!pip install scikit-learn\n",
    "!pip install --upgrade simctg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0,'/content/drive/MyDrive/1_Studium/01_MasterThesis/resources/')\n",
    "sys.path.append('/content/gdrive/My Drive/1_Studium/01_MasterThesis/resources')\n",
    "\n",
    "!ls \"/content/drive/MyDrive/1_Studium/01_MasterThesis/resources/\"\n",
    "\n",
    "!cp \"/content/drive/My Drive/1_Studium/01_MasterThesis/resources/metrics.py\" \"/content/metrics.py\"\n",
    "!cp \"/content/drive/My Drive/1_Studium/01_MasterThesis/resources/utils.py\" \"/content/utils.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch.utils.data\n",
    "from transformers import GPT2TokenizerFast, GPT2Config, AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import torch\n",
    "from transformers import TrainerCallback, TrainerState, TrainerControl\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "from utils import device, NewsData, CombinedDataset, gen_and_eval, predict_text_proba, calculate_model_ppls_samplewise\n",
    "#from simctg.lossfunction import SimCTGLoss\n",
    "from tokenizers import Tokenizer\n",
    "import metrics\n",
    "\n",
    "from metrics import monolingual_easy_language_german\n",
    "\n",
    "from datasets import load_from_disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Runinng on device: \", device)\n",
    "\n",
    "#results_path = \"/content/drive/My Drive/1_Studium/01_MasterThesis/01_Python/result\"\n",
    "results_path = '../01_Python/result'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_string = \"dbmdz/german-gpt2\"\n",
    "#base_model_string = \"benjamin/gpt2-wechsel-german\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComplexityCallback(TrainerCallback):\n",
    "\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.metrics = pd.DataFrame(\n",
    "            columns=[\"steps\", \"fre\", \"fkgl\", \"wiener\", \"avg_word_length\", \"avg_sentence_length\", \"words_per_sentence\",\n",
    "                     \"avg_syllables_per_word\", \"polysyllables\", \"text\"])\n",
    "\n",
    "    def on_train_begin(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs):\n",
    "        self.on_evaluate(args=args, state=state, control=control, kwargs=kwargs)\n",
    "\n",
    "    def on_evaluate(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs):\n",
    "        print('*Entered evaluation callback')\n",
    "        #        encoding = torch.tensor([[self.tokenizer.bos_token_id]]).to(device)\n",
    "        encoding = tokenizer(\"Dieses Haus \", return_tensors=\"pt\")['input_ids'].to(device)\n",
    "        #        pred_ids = model.generate(encoding, max_length=128, top_k=5, top_p=0.92, do_sample=True, temperature=0.7, num_return_sequences=3)\n",
    "        pred_ids = model.generate(encoding, max_length=128, top_k=4, penalty_alpha=0.6, repetition_penalty=1.4)\n",
    "        pred_sents = self.tokenizer.batch_decode(pred_ids)[0]\n",
    "        scores = monolingual_easy_language_german(pred_sents)\n",
    "        scores['steps'] = state.global_step\n",
    "        scores['text'] = pred_sents\n",
    "        self.metrics = self.metrics.append(scores, ignore_index=True)\n",
    "\n",
    "\n",
    "margin = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveTrainer(Trainer):\n",
    "\n",
    "    def __init__(self, tokenizer, **kwargs):\n",
    "        self.vocab_size = len(tokenizer)\n",
    "        self.pad_token_id = tokenizer.pad_token_id\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        input_ids = inputs.get('input_ids')\n",
    "        labels = torch.roll(inputs.get('labels'), 1)\n",
    "\n",
    "        # forward computation\n",
    "        bsz, seqlen = input_ids.size()\n",
    "        outputs = model(**inputs, output_hidden_states=True)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        regular_loss = outputs.loss\n",
    "        if self.label_smoother is not None:\n",
    "            regular_loss = self.label_smoother(outputs, inputs.get(\"labels\"), shift_labels=True)\n",
    "\n",
    "        assert logits.size() == torch.Size([bsz, seqlen, model.config.vocab_size])\n",
    "        last_hidden_states = outputs.hidden_states[-1]\n",
    "\n",
    "        # compute cl loss\n",
    "        norm_rep = last_hidden_states / last_hidden_states.norm(dim=2, keepdim=True)\n",
    "        cosine_scores = torch.matmul(norm_rep, norm_rep.transpose(1, 2))\n",
    "        assert cosine_scores.size() == torch.Size([bsz, seqlen, seqlen])\n",
    "        simctgloss = SimCTGLoss(margin=margin, vocab_size=self.vocab_size, pad_token_id=self.pad_token_id)\n",
    "        cl_loss = simctgloss.contrastive_loss(cosine_scores, input_ids)\n",
    "\n",
    "        simctg_loss = regular_loss + cl_loss\n",
    "        return (simctg_loss, logits) if return_outputs else simctg_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Finetuning\", base_model_string)\n",
    "base_model_name = base_model_string.split('/')[-1]\n",
    "\n",
    "# the eos and bos tokens are defined\n",
    "bos = '<|bos|>'\n",
    "eos = '<|eos|>'\n",
    "pad = '<|pad|>'\n",
    "special_tokens_dict = {'eos_token': eos, 'bos_token': bos, 'pad_token': pad}\n",
    "\n",
    "tokenizer_orig = AutoTokenizer.from_pretrained(base_model_string)\n",
    "tokenizer_orig.add_special_tokens(special_tokens_dict)\n",
    "tokenizer = Tokenizer.from_pretrained(base_model_string)\n",
    "tokenizer.post_processor = TemplateProcessing(\n",
    "    single=bos + \" $A \" + eos,\n",
    "    special_tokens=[(eos, tokenizer_orig.eos_token_id), (bos, tokenizer_orig.bos_token_id)],\n",
    ")\n",
    "tokenizer = GPT2TokenizerFast(tokenizer_object=tokenizer)\n",
    "num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "\n",
    "configuration = GPT2Config.from_pretrained(base_model_string, bos_token_id=tokenizer.bos_token_id,\n",
    "                                eos_token_id=tokenizer.eos_token_id,\n",
    "                                pad_token_id=tokenizer.pad_token_id,\n",
    "                                #use_cache=False,\n",
    "                                )\n",
    "configuration.embd_pdrop = 0.1 #hyperparameter\n",
    "configuration.attn_pdrop = 0.1 #hyperparameter\n",
    "configuration.resid_pdrop = 0.1 #hyperparameter\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model_string, config=configuration, force_download=True)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 128\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = examples['sents_standard']\n",
    "    targets = examples['sents_leicht']\n",
    "    model_inputs = tokenizer(\n",
    "        inputs, text_target=targets, max_length=max_length, padding=True, truncation=True\n",
    "    )\n",
    "    model_inputs[\"labels\"] = examples[\"id\"]\n",
    "\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stride_length = 64\n",
    "max_length = model.config.n_positions\n",
    "#dataset = load_from_disk(\"/content/drive/My Drive/1_Studium/01_MasterThesis/01_Python/COMPLETE_set_corrected\")\n",
    "dataset = load_from_disk('COMPLETE_SET_corrected')\n",
    "print(dataset)\n",
    "tokenized_datasets = dataset.map(\n",
    "  preprocess_function,\n",
    "  batched=True,\n",
    "  remove_columns=dataset[\"train\"].column_names,\n",
    ")\n",
    "\n",
    "\n",
    "#generator = torch.Generator()\n",
    "\n",
    "#test_val_length = int(.1 * len(dataset))\n",
    "#train_length = len(dataset) - test_val_length\n",
    "#train_set, val_set = torch.utils.data.random_split(dataset, [train_length, test_val_length],\n",
    "#                                                             generator=generator)\n",
    "\n",
    "#print(dataset.get_summary())\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "eval_steps = 100\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    num_train_epochs=1,\n",
    "    output_dir=results_path+base_model_name,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy='epoch',\n",
    "    learning_rate=1e-4,  # hyperparamater\n",
    "    weight_decay=0.01,  # hyperparamater\n",
    "    #per_device_train_batch_size=1,\n",
    "    auto_find_batch_size=True,\n",
    "    gradient_accumulation_steps=4,\n",
    "    #gradient_checkpointing=True,\n",
    "    warmup_steps=200,\n",
    "    logging_steps=eval_steps,\n",
    "    eval_steps=eval_steps,\n",
    "    #eval_accumulation_steps=1,\n",
    "    fp16=True if device != 'cpu' else False,\n",
    "    #push_to_hub=True,\n",
    "    #hub_model_id=base_model_name+'_easy'\n",
    ")\n",
    "\n",
    "#trainer = ContrastiveTrainer(\n",
    "trainer = Trainer(\n",
    "    tokenizer=tokenizer,\n",
    "    model=model.to(device),\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.add_callback(ComplexityCallback(tokenizer))\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Saving tokenizer\")\n",
    "#tokenizer.save_pretrained(\"/content/drive/My Drive/1_Studium/01_MasterThesis/01_Python/COMPLETE_set_standard_\"+base_model_name)\n",
    "tokenizer.save_pretrained(\"COMPLETE_set_standard_\"+base_model_name)\n",
    "#model.save_pretrained(\"/content/drive/My Drive/1_Studium/01_MasterThesis/01_Python/trainer_pretrained_COMPLETE_set_standard_\"+base_model_name)\n",
    "model.save_pretrained(\"trainer_pretrained_COMPLETE_set_standard_\"+base_model_name)\n",
    "#trainer.push_to_hub()\n",
    "print(\"Saving complexity history\")\n",
    "complexity_history = trainer.pop_callback(ComplexityCallback)\n",
    "#complexity_history.metrics.to_csv(\"/content/drive/My Drive/1_Studium/01_MasterThesis/01_Python/COMPLETE_set_standard_\"+base_model_name + '/complexity.csv', index=False)\n",
    "complexity_history.metrics.to_csv(\"COMPLETE_set_standard_\"+base_model_name + '/complexity.csv', index=False)\n",
    "\n",
    "model_orig = AutoModelForCausalLM.from_pretrained(base_model_string)\n",
    "tokenizer_orig = AutoTokenizer.from_pretrained(base_model_string)\n",
    "model_orig.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open(\"/content/drive/My Drive/1_Studium/01_MasterThesis/01_Python/COMPLETE_set_standard_\"+base_model_name + '/metrics.txt', 'w+', encoding='utf-8') as outfile:\n",
    "with open(\"COMPLETE_set_standard_\"+base_model_name + '/metrics.txt', 'w+', encoding='utf-8') as outfile:\n",
    "  outfile.write(f'Comparing: %s' %base_model_string)\n",
    "  input = [\"Die Türkei\"]\n",
    "  outfile.write(\"\\nOriginal GPT\")\n",
    "  outfile.write(str(gen_and_eval(input, model_orig.eval(), tokenizer_orig)))\n",
    "  outfile.write(\"\\nFine-tuned GPT\")\n",
    "  outfile.write(str(gen_and_eval(input, model.eval(), tokenizer)))\n",
    "\n",
    "\n",
    "  text_easy = \"Leichte Sprache ist leichter zu lesen.\"\n",
    "  text_complex = \"Leichte Sprache ist eine speziell geregelte einfache Sprache.\"\n",
    "  outfile.write(\"\\n\\nEasy text sample\")\n",
    "  outfile.write(f\"\\nOriginal GPT: {predict_text_proba(text_easy, model_orig.eval(), tokenizer_orig)}\")\n",
    "  outfile.write(f\"\\nFine-tuned GPT: {predict_text_proba(text_easy, model.eval(), tokenizer)}\")\n",
    "  outfile.write(\"\\nComplex text sample\")\n",
    "  outfile.write(f\"\\nOriginal GPT: {predict_text_proba(text_complex, model_orig.eval(), tokenizer_orig)}\")\n",
    "  outfile.write(f\"\\nFine-tuned GPT {predict_text_proba(text_complex, model.eval(), tokenizer)}\")\n",
    "\n",
    "  outfile.write('\\n\\n Perplexity')\n",
    "  #koeln = pd.read_csv(\"/content/drive/My Drive/1_Studium/01_MasterThesis/02_Data/P_koeln.csv\", index_col=[0])\n",
    "  koeln = pd.read_csv(\"../../02_Data/P_koeln.csv\", index_col=[0])\n",
    "  simple_texts = koeln.dropna(subset=['text_leicht'])['text_leicht'].values.tolist()\n",
    "  normal_texts = koeln.dropna(subset=['text_normal'])['text_normal'].values.tolist()\n",
    "  simp_ppl, norm_ppl, _, _ = calculate_model_ppls_samplewise(model, tokenizer, simple_texts, normal_texts)\n",
    "  outfile.write(f\"\\nPerplexity simple fine-tuned GPT: %f\" %simp_ppl)\n",
    "  outfile.write(f\"\\nPerplexity normal fine-tuned GPT: %f\" %norm_ppl)\n",
    "  simp_ppl_orig, norm_ppl_orig, _, _ = calculate_model_ppls_samplewise(model_orig, tokenizer_orig, simple_texts, normal_texts)\n",
    "  outfile.write(f\"\\nPerplexity simple original GPT: %f\" %simp_ppl_orig)\n",
    "  outfile.write(f\"\\nPerplexity normal original GPT: %f\" %norm_ppl_orig)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
